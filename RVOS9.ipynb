{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wizardoftrap/Referring-Video-Object-Segmentation/blob/main/RVOS9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***RVOS-9:***\n",
        "###***Shiv Prakash Verma(2021eeb1030)***"
      ],
      "metadata": {
        "id": "LZHMuvOyI44Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7Qa8H1GCXW4F"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision\n",
        "!pip install opencv-python\n",
        "!pip install transformers\n",
        "!pip install matplotlib\n",
        "!pip install tqdm\n",
        "!pip install pycocotools\n",
        "!pip install einops\n",
        "!pip install albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TRri-KSL1juH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
        "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"CUDA not available, using CPU\")\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhSyLg4fXjvO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZAWoMXYra9FD"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/drive/MyDrive/RVOS_VR_Project\n",
        "!mkdir /content/drive/MyDrive/RVOS_VR_Project/data\n",
        "!mkdir /content/drive/MyDrive/RVOS_VR_Project/data/ref-davis\n",
        "\n",
        "%cd /content/drive/MyDrive/RVOS_VR_Project/data/ref-davis\n",
        "\n",
        "#Download DAVIS dataset files\n",
        "!wget -c https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-Unsupervised-trainval-480p.zip\n",
        "!wget -c https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017_semantics-480p.zip\n",
        "\n",
        "#Download text annotations\n",
        "!wget -c https://www.mpi-inf.mpg.de/fileadmin/inf/d2/khoreva/davis_text_annotations.zip\n",
        "\n",
        "!unzip -o davis_text_annotations.zip\n",
        "\n",
        "#%cd /content/drive/MyDrive/RVOS_VR_Project"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o DAVIS-2017_semantics-480p.zip\n",
        "#%cd /content/drive/MyDrive/RVOS_VR_Project"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ORfX1ghjImow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o DAVIS-2017-Unsupervised-trainval-480p.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_6ygrJ_3Ou_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd /content/drive/MyDrive/RVOS_VR_Project"
      ],
      "metadata": {
        "id": "iScq23V_O2JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OyVjgAtkn2Mu"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/MyDrive/RVOS_VR_Project/data/ref-davis/DAVIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQaL2xF0ftyi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def parse_davis_text_annotations(annotation_file):\n",
        "    \"\"\"Parse DAVIS text annotations from the given file.\"\"\"\n",
        "    annotations = {}\n",
        "\n",
        "    with open(annotation_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line or line.startswith('%%%'):  #skip empty lines and format description\n",
        "            continue\n",
        "\n",
        "        #parse the line: video_name object_id \"referring_expression\"\n",
        "        parts = line.split('\"')\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "\n",
        "        prefix = parts[0].strip().split()\n",
        "        if len(prefix) < 2:\n",
        "            continue\n",
        "\n",
        "        video_name = prefix[0]\n",
        "        obj_id = prefix[1]\n",
        "        expression = parts[1]\n",
        "\n",
        "        #initialize video entry if not exists\n",
        "        if video_name not in annotations:\n",
        "            annotations[video_name] = []\n",
        "\n",
        "        #add the annotation\n",
        "        annotations[video_name].append({\n",
        "            'obj_id': obj_id,\n",
        "            'expression': expression\n",
        "        })\n",
        "\n",
        "    return annotations\n",
        "\n",
        "#Parse the annotations\n",
        "davis17_annot1 = parse_davis_text_annotations('/content/drive/MyDrive/RVOS_VR_Project/data/ref-davis/davis_text_annotations/Davis17_annot1.txt')\n",
        "davis17_annot2 = parse_davis_text_annotations('/content/drive/MyDrive/RVOS_VR_Project/data/ref-davis/davis_text_annotations/Davis17_annot2.txt')\n",
        "\n",
        "#Combine annotations\n",
        "davis17_annotations = {}\n",
        "for video_name, annotations in davis17_annot1.items():\n",
        "    davis17_annotations[video_name] = annotations\n",
        "\n",
        "for video_name, annotations in davis17_annot2.items():\n",
        "    if video_name in davis17_annotations:\n",
        "        davis17_annotations[video_name].extend(annotations)\n",
        "    else:\n",
        "        davis17_annotations[video_name] = annotations\n",
        "\n",
        "#print statistics\n",
        "print(f\"Total videos: {len(davis17_annotations)}\")\n",
        "total_expressions = sum(len(annotations) for annotations in davis17_annotations.values())\n",
        "print(f\"Total expressions: {total_expressions}\")\n",
        "\n",
        "#save the combined annotations for easier access\n",
        "with open('/content/drive/MyDrive/RVOS_VR_Project/data/ref-davis/combined_annotations.json', 'w') as f:\n",
        "    json.dump(davis17_annotations, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MQVDKbERqW7l"
      },
      "outputs": [],
      "source": [
        "def visualize_sample(data_root, annotations):\n",
        "    \"\"\"Visualize a random sample from the dataset.\"\"\"\n",
        "    video_names = list(annotations.keys())\n",
        "    random_video = random.choice(video_names)\n",
        "\n",
        "    #get expressions for this video\n",
        "    expressions = annotations[random_video]\n",
        "    random_exp = random.choice(expressions)\n",
        "\n",
        "    #get the object ID and expression\n",
        "    obj_id = random_exp['obj_id']\n",
        "    expression_text = random_exp['expression']\n",
        "\n",
        "    print(f\"Selected video: {random_video}\")\n",
        "    print(f\"Object ID from annotation: {obj_id}\")\n",
        "    print(f\"Expression: {expression_text}\")\n",
        "\n",
        "    #get frame paths\n",
        "    frames_dir = os.path.join(data_root, 'DAVIS', 'JPEGImages', '480p', random_video)\n",
        "    masks_dir = os.path.join(data_root, 'DAVIS', 'Annotations_unsupervised', '480p', random_video)\n",
        "\n",
        "    if not os.path.exists(frames_dir):\n",
        "        print(f\"Frames directory not found: {frames_dir}\")\n",
        "        return None, None, None\n",
        "\n",
        "    if not os.path.exists(masks_dir):\n",
        "        print(f\"Masks directory not found: {masks_dir}\")\n",
        "        return None, None, None\n",
        "\n",
        "    print(f\"Using frames directory: {frames_dir}\")\n",
        "    print(f\"Using masks directory: {masks_dir}\")\n",
        "\n",
        "    #get all frames\n",
        "    frame_files = sorted([f for f in os.listdir(frames_dir) if f.endswith('.jpg')])\n",
        "\n",
        "    if not frame_files:\n",
        "        print(f\"No frame files found in {frames_dir}\")\n",
        "        return None, None, None\n",
        "\n",
        "    #sample a few frames\n",
        "    sample_indices = [0, len(frame_files)//2, len(frame_files)-1]  # First, middle, last\n",
        "\n",
        "    #first, let's analyze the masks to find all available object IDs\n",
        "    mask_file = frame_files[0].replace('.jpg', '.png')\n",
        "    mask_path = os.path.join(masks_dir, mask_file)\n",
        "\n",
        "    if os.path.exists(mask_path):\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        unique_values = np.unique(mask)\n",
        "        print(f\"Available object IDs in mask: {unique_values}\")\n",
        "\n",
        "        #filter out background (0)\n",
        "        object_ids = [id for id in unique_values if id > 0]\n",
        "\n",
        "        if not object_ids:\n",
        "            print(\"No object IDs found in mask!\")\n",
        "            return None, None, None\n",
        "\n",
        "        #for now, let's just use the first non-zero ID\n",
        "        #in a real application, you'd need to map between text annotation IDs and mask IDs\n",
        "        actual_obj_id = object_ids[0]\n",
        "        print(f\"Using object ID {actual_obj_id} from mask (instead of {obj_id} from annotation)\")\n",
        "    else:\n",
        "        print(f\"Mask file not found: {mask_path}\")\n",
        "        return None, None, None\n",
        "\n",
        "    fig, axes = plt.subplots(len(sample_indices), 3, figsize=(15, 4*len(sample_indices)))\n",
        "\n",
        "    for i, idx in enumerate(sample_indices):\n",
        "        frame_file = frame_files[idx]\n",
        "\n",
        "        #load frame\n",
        "        frame_path = os.path.join(frames_dir, frame_file)\n",
        "        frame = cv2.imread(frame_path)\n",
        "        if frame is None:\n",
        "            print(f\"Failed to load frame: {frame_path}\")\n",
        "            continue\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        #load mask\n",
        "        mask_file = frame_file.replace('.jpg', '.png')\n",
        "        mask_path = os.path.join(masks_dir, mask_file)\n",
        "\n",
        "        if not os.path.exists(mask_path):\n",
        "            print(f\"Mask file not found: {mask_path}\")\n",
        "            obj_mask = np.zeros((frame.shape[0], frame.shape[1]), dtype=np.float32)\n",
        "        else:\n",
        "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "            if mask is None:\n",
        "                print(f\"Failed to load mask: {mask_path}\")\n",
        "                obj_mask = np.zeros((frame.shape[0], frame.shape[1]), dtype=np.float32)\n",
        "            else:\n",
        "                obj_mask = (mask == actual_obj_id).astype(np.float32)\n",
        "\n",
        "        #original frame\n",
        "        axes[i, 0].imshow(frame)\n",
        "        axes[i, 0].set_title(f\"Frame {idx}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        #mask only\n",
        "        axes[i, 1].imshow(obj_mask, cmap='gray')\n",
        "        axes[i, 1].set_title(f\"Mask (Object ID: {actual_obj_id})\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        #overlay\n",
        "        masked_img = frame.copy()\n",
        "        mask_colored = np.zeros_like(frame)\n",
        "        mask_colored[:,:,0] = obj_mask * 255  # Red channel\n",
        "        masked_img = cv2.addWeighted(masked_img, 1, mask_colored, 0.5, 0)\n",
        "\n",
        "        axes[i, 2].imshow(masked_img)\n",
        "        axes[i, 2].set_title(f\"Overlay: {expression_text}\")\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    #second object ID if available\n",
        "    if len(object_ids) > 1:\n",
        "        actual_obj_id = object_ids[1]\n",
        "        print(f\"\\nTrying second object ID: {actual_obj_id}\")\n",
        "\n",
        "        fig, axes = plt.subplots(len(sample_indices), 3, figsize=(15, 4*len(sample_indices)))\n",
        "\n",
        "        for i, idx in enumerate(sample_indices):\n",
        "            frame_file = frame_files[idx]\n",
        "\n",
        "            #load frame\n",
        "            frame_path = os.path.join(frames_dir, frame_file)\n",
        "            frame = cv2.imread(frame_path)\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            #load mask\n",
        "            mask_file = frame_file.replace('.jpg', '.png')\n",
        "            mask_path = os.path.join(masks_dir, mask_file)\n",
        "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "            obj_mask = (mask == actual_obj_id).astype(np.float32)\n",
        "\n",
        "            #original frame\n",
        "            axes[i, 0].imshow(frame)\n",
        "            axes[i, 0].set_title(f\"Frame {idx}\")\n",
        "            axes[i, 0].axis('off')\n",
        "\n",
        "            #mask only\n",
        "            axes[i, 1].imshow(obj_mask, cmap='gray')\n",
        "            axes[i, 1].set_title(f\"Mask (Object ID: {actual_obj_id})\")\n",
        "            axes[i, 1].axis('off')\n",
        "\n",
        "            #overlay\n",
        "            masked_img = frame.copy()\n",
        "            mask_colored = np.zeros_like(frame)\n",
        "            mask_colored[:,:,0] = obj_mask * 255\n",
        "            masked_img = cv2.addWeighted(masked_img, 1, mask_colored, 0.5, 0)\n",
        "\n",
        "            axes[i, 2].imshow(masked_img)\n",
        "            axes[i, 2].set_title(f\"Overlay: {expression_text}\")\n",
        "            axes[i, 2].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return random_video, expression_text, obj_id\n",
        "\n",
        "try:\n",
        "    sample_video, sample_expr, sample_obj_id = visualize_sample(\n",
        "        '/content/drive/MyDrive/RVOS_VR_Project/data/ref-davis',\n",
        "        davis17_annotations\n",
        "    )\n",
        "    if sample_video:\n",
        "        print(f\"Video: {sample_video}, Expression: '{sample_expr}', Object ID: {sample_obj_id}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error visualizing sample: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hc2dmlqTsVZR"
      },
      "outputs": [],
      "source": [
        "#mapping between annotation object IDs and mask object IDs\n",
        "def create_object_id_mapping(annotations):\n",
        "    \"\"\"Create a mapping between annotation object IDs and actual mask object IDs.\"\"\"\n",
        "    davis_root = '/content/drive/MyDrive/RVOS_VR_Project/data/ref-davis'\n",
        "    frames_dir = os.path.join(davis_root, 'DAVIS', 'JPEGImages', '480p')\n",
        "    masks_dir = os.path.join(davis_root, 'DAVIS', 'Annotations_unsupervised', '480p')\n",
        "\n",
        "    id_mapping = {}\n",
        "\n",
        "    for video_name, expressions in annotations.items():\n",
        "        if not os.path.exists(os.path.join(frames_dir, video_name)) or not os.path.exists(os.path.join(masks_dir, video_name)):\n",
        "            continue\n",
        "\n",
        "        #frame files\n",
        "        frame_files = sorted([f for f in os.listdir(os.path.join(frames_dir, video_name)) if f.endswith('.jpg')])\n",
        "        if not frame_files:\n",
        "            continue\n",
        "\n",
        "        #load first mask to get object IDs\n",
        "        mask_file = frame_files[0].replace('.jpg', '.png')\n",
        "        mask_path = os.path.join(masks_dir, video_name, mask_file)\n",
        "\n",
        "        if not os.path.exists(mask_path):\n",
        "            continue\n",
        "\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if mask is None:\n",
        "            continue\n",
        "\n",
        "        #unique object IDs in mask - convert to standard Python int\n",
        "        unique_ids = [int(id) for id in np.unique(mask) if id > 0]\n",
        "\n",
        "        #mapping for this video\n",
        "        id_mapping[video_name] = {}\n",
        "\n",
        "        #for each expression, assign the first available object ID\n",
        "        for i, exp_data in enumerate(expressions):\n",
        "            anno_obj_id = exp_data['obj_id']\n",
        "            if i < len(unique_ids):\n",
        "                id_mapping[video_name][anno_obj_id] = unique_ids[i]\n",
        "            else:\n",
        "                # If we run out of object IDs, use the first one\n",
        "                id_mapping[video_name][anno_obj_id] = unique_ids[0] if unique_ids else 0\n",
        "\n",
        "    return id_mapping\n",
        "\n",
        "def split_dataset(annotations, train_ratio=0.8, seed=42):\n",
        "    \"\"\"Split the dataset into training and validation sets.\"\"\"\n",
        "    random.seed(seed)\n",
        "\n",
        "    #list of videos that actually exist in the dataset\n",
        "    davis_root = '/content/drive/MyDrive/RVOS_VR_Project/data/ref-davis'\n",
        "    frames_dir = os.path.join(davis_root, 'DAVIS', 'JPEGImages', '480p')\n",
        "    masks_dir = os.path.join(davis_root, 'DAVIS', 'Annotations_unsupervised', '480p')\n",
        "\n",
        "    #check which videos from annotations exist in the dataset\n",
        "    available_videos = []\n",
        "    for video_name in annotations.keys():\n",
        "        if os.path.exists(os.path.join(frames_dir, video_name)) and os.path.exists(os.path.join(masks_dir, video_name)):\n",
        "            available_videos.append(video_name)\n",
        "\n",
        "    print(f\"Total videos in annotations: {len(annotations)}\")\n",
        "    print(f\"Available videos in dataset: {len(available_videos)}\")\n",
        "\n",
        "    #shuffle and split\n",
        "    random.shuffle(available_videos)\n",
        "    split_idx = int(len(available_videos) * train_ratio)\n",
        "    train_videos = available_videos[:split_idx]\n",
        "    val_videos = available_videos[split_idx:]\n",
        "\n",
        "    train_annotations = {video: annotations[video] for video in train_videos}\n",
        "    val_annotations = {video: annotations[video] for video in val_videos}\n",
        "\n",
        "    return train_annotations, val_annotations\n",
        "\n",
        "#split the dataset\n",
        "train_annotations, val_annotations = split_dataset(davis17_annotations)\n",
        "\n",
        "print(f\"Training videos: {len(train_annotations)}\")\n",
        "print(f\"Validation videos: {len(val_annotations)}\")\n",
        "\n",
        "#save the splits\n",
        "with open('/content/drive/MyDrive/RVOS_VR_Project/data/ref-davis/train_annotations.json', 'w') as f:\n",
        "    json.dump(train_annotations, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/RVOS_VR_Project/data/ref-davis/val_annotations.json', 'w') as f:\n",
        "    json.dump(val_annotations, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "47ej6Vbiuuxd"
      },
      "outputs": [],
      "source": [
        "def visualize_with_mapping(data_root, annotations, id_mapping, video_name=None):\n",
        "    \"\"\"Visualize a sample using the object ID mapping.\"\"\"\n",
        "    if video_name is None:\n",
        "        #random video that has a mapping\n",
        "        mapped_videos = list(id_mapping.keys())\n",
        "        if not mapped_videos:\n",
        "            print(\"No videos with ID mapping available\")\n",
        "            return None, None, None\n",
        "        video_name = random.choice(mapped_videos)\n",
        "\n",
        "    if video_name not in annotations:\n",
        "        print(f\"Video {video_name} not in annotations\")\n",
        "        return None, None, None\n",
        "\n",
        "    #expressions for this video\n",
        "    expressions = annotations[video_name]\n",
        "    random_exp = random.choice(expressions)\n",
        "\n",
        "    #the object ID and expression\n",
        "    anno_obj_id = random_exp['obj_id']\n",
        "    expression_text = random_exp['expression']\n",
        "\n",
        "    #the mapped object ID\n",
        "    if video_name in id_mapping and anno_obj_id in id_mapping[video_name]:\n",
        "        actual_obj_id = id_mapping[video_name][anno_obj_id]\n",
        "    else:\n",
        "        print(f\"No mapping found for video {video_name}, object ID {anno_obj_id}\")\n",
        "        return None, None, None\n",
        "\n",
        "    print(f\"Selected video: {video_name}\")\n",
        "    print(f\"Annotation object ID: {anno_obj_id}\")\n",
        "    print(f\"Mapped to actual object ID: {actual_obj_id}\")\n",
        "    print(f\"Expression: {expression_text}\")\n",
        "\n",
        "    #frame paths\n",
        "    frames_dir = os.path.join(data_root, 'DAVIS', 'JPEGImages', '480p', video_name)\n",
        "    masks_dir = os.path.join(data_root, 'DAVIS', 'Annotations_unsupervised', '480p', video_name)\n",
        "\n",
        "    if not os.path.exists(frames_dir) or not os.path.exists(masks_dir):\n",
        "        print(f\"Directories not found for video {video_name}\")\n",
        "        return None, None, None\n",
        "\n",
        "    #all frames\n",
        "    frame_files = sorted([f for f in os.listdir(frames_dir) if f.endswith('.jpg')])\n",
        "\n",
        "    if not frame_files:\n",
        "        print(f\"No frame files found for video {video_name}\")\n",
        "        return None, None, None\n",
        "\n",
        "    #sample a few frames\n",
        "    sample_indices = [0, len(frame_files)//2, len(frame_files)-1]  # First, middle, last\n",
        "\n",
        "    fig, axes = plt.subplots(len(sample_indices), 3, figsize=(15, 4*len(sample_indices)))\n",
        "\n",
        "    for i, idx in enumerate(sample_indices):\n",
        "        frame_file = frame_files[idx]\n",
        "\n",
        "        #frame\n",
        "        frame_path = os.path.join(frames_dir, frame_file)\n",
        "        frame = cv2.imread(frame_path)\n",
        "        if frame is None:\n",
        "            print(f\"Failed to load frame: {frame_path}\")\n",
        "            continue\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        #mask\n",
        "        mask_file = frame_file.replace('.jpg', '.png')\n",
        "        mask_path = os.path.join(masks_dir, mask_file)\n",
        "\n",
        "        if not os.path.exists(mask_path):\n",
        "            print(f\"Mask file not found: {mask_path}\")\n",
        "            obj_mask = np.zeros((frame.shape[0], frame.shape[1]), dtype=np.float32)\n",
        "        else:\n",
        "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "            if mask is None:\n",
        "                print(f\"Failed to load mask: {mask_path}\")\n",
        "                obj_mask = np.zeros((frame.shape[0], frame.shape[1]), dtype=np.float32)\n",
        "            else:\n",
        "                #use the mapped object ID\n",
        "                obj_mask = (mask == actual_obj_id).astype(np.float32)\n",
        "\n",
        "        #original frame\n",
        "        axes[i, 0].imshow(frame)\n",
        "        axes[i, 0].set_title(f\"Frame {idx}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        #mask only\n",
        "        axes[i, 1].imshow(obj_mask, cmap='gray')\n",
        "        axes[i, 1].set_title(f\"Mask (Object ID: {actual_obj_id})\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        #overlay\n",
        "        masked_img = frame.copy()\n",
        "        mask_colored = np.zeros_like(frame)\n",
        "        mask_colored[:,:,0] = obj_mask * 255\n",
        "        masked_img = cv2.addWeighted(masked_img, 1, mask_colored, 0.5, 0)\n",
        "\n",
        "        axes[i, 2].imshow(masked_img)\n",
        "        axes[i, 2].set_title(f\"Overlay: {expression_text}\")\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return video_name, expression_text, anno_obj_id\n",
        "#create object ID mapping\n",
        "object_id_mapping = create_object_id_mapping(davis17_annotations)\n",
        "#visualize a sample from the training set\n",
        "print(\"\\nVisualizing a sample from the training set:\")\n",
        "train_video, train_expr, train_obj_id = visualize_with_mapping(\n",
        "    '/content/drive/MyDrive/RVOS_VR_Project/data/ref-davis',\n",
        "    train_annotations,\n",
        "    object_id_mapping\n",
        ")\n",
        "\n",
        "#visualize a sample from the validation set\n",
        "print(\"\\nVisualizing a sample from the validation set:\")\n",
        "val_video, val_expr, val_obj_id = visualize_with_mapping(\n",
        "    '/content/drive/MyDrive/RVOS_VR_Project/data/ref-davis',\n",
        "    val_annotations,\n",
        "    object_id_mapping\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zTWOE3atvETB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import BertTokenizer\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "class RefDAVISDataset(Dataset):\n",
        "    def __init__(self, data_root, annotations, id_mapping, transform=None, max_seq_len=5):\n",
        "        self.data_root = data_root\n",
        "        self.annotations = annotations\n",
        "        self.id_mapping = id_mapping\n",
        "        self.transform = transform\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        #samples list\n",
        "        self.samples = []\n",
        "        for video_id, expressions in self.annotations.items():\n",
        "            #skip videos without ID mapping\n",
        "            if video_id not in self.id_mapping:\n",
        "                continue\n",
        "\n",
        "            for exp_data in expressions:\n",
        "                obj_id = exp_data['obj_id']\n",
        "                #skip objects without ID mapping\n",
        "                if obj_id not in self.id_mapping[video_id]:\n",
        "                    continue\n",
        "\n",
        "                expression = exp_data['expression']\n",
        "                self.samples.append({\n",
        "                    'video_id': video_id,\n",
        "                    'expression': expression,\n",
        "                    'obj_id': obj_id\n",
        "                })\n",
        "\n",
        "        print(f\"Created dataset with {len(self.samples)} samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        video_id = sample['video_id']\n",
        "        anno_obj_id = sample['obj_id']\n",
        "        expression = sample['expression']\n",
        "\n",
        "        #the mapped object ID\n",
        "        actual_obj_id = self.id_mapping[video_id][anno_obj_id]\n",
        "\n",
        "        #frame paths\n",
        "        frames_dir = os.path.join(self.data_root, 'DAVIS', 'JPEGImages', '480p', video_id)\n",
        "        masks_dir = os.path.join(self.data_root, 'DAVIS', 'Annotations_unsupervised', '480p', video_id)\n",
        "\n",
        "        #check if directories exist\n",
        "        if not os.path.exists(frames_dir) or not os.path.exists(masks_dir):\n",
        "            #return dummy data if directories don't exist\n",
        "            dummy_frame = torch.zeros(3, 384, 384)\n",
        "            dummy_mask = torch.zeros(384, 384)\n",
        "            dummy_text_ids = torch.zeros(20, dtype=torch.long)\n",
        "            dummy_text_mask = torch.zeros(20, dtype=torch.long)\n",
        "\n",
        "            return {\n",
        "                'frames': torch.stack([dummy_frame] * self.max_seq_len),\n",
        "                'masks': torch.stack([dummy_mask] * self.max_seq_len),\n",
        "                'text_ids': dummy_text_ids,\n",
        "                'text_mask': dummy_text_mask,\n",
        "                'expression': expression,\n",
        "                'video_id': video_id,\n",
        "                'obj_id': anno_obj_id\n",
        "            }\n",
        "\n",
        "        frame_files = sorted([f for f in os.listdir(frames_dir) if f.endswith('.jpg')])\n",
        "\n",
        "        #sample frames (for simplicity, take evenly spaced frames)\n",
        "        if len(frame_files) > self.max_seq_len:\n",
        "            indices = np.linspace(0, len(frame_files)-1, self.max_seq_len, dtype=int)\n",
        "            frame_files = [frame_files[i] for i in indices]\n",
        "        else:\n",
        "            # If fewer frames than max_seq_len, use all and pad later\n",
        "            frame_files = frame_files[:self.max_seq_len]\n",
        "\n",
        "        #load frames and masks\n",
        "        frames = []\n",
        "        masks = []\n",
        "\n",
        "        for frame_file in frame_files:\n",
        "            #frame\n",
        "            frame_path = os.path.join(frames_dir, frame_file)\n",
        "            frame = cv2.imread(frame_path)\n",
        "            if frame is None:\n",
        "                #use a blank frame if loading fails\n",
        "                frame = np.zeros((384, 384, 3), dtype=np.uint8)\n",
        "            else:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            #load mask\n",
        "            mask_file = frame_file.replace('.jpg', '.png')\n",
        "            mask_path = os.path.join(masks_dir, mask_file)\n",
        "\n",
        "            if not os.path.exists(mask_path):\n",
        "                #use a blank mask if file doesn't exist\n",
        "                obj_mask = np.zeros((frame.shape[0], frame.shape[1]), dtype=np.float32)\n",
        "            else:\n",
        "                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "                if mask is None:\n",
        "                    #use a blank mask if loading fails\n",
        "                    obj_mask = np.zeros((frame.shape[0], frame.shape[1]), dtype=np.float32)\n",
        "                else:\n",
        "                    #extract mask for the specific object using the mapped ID\n",
        "                    obj_mask = (mask == actual_obj_id).astype(np.float32)\n",
        "\n",
        "            if self.transform:\n",
        "                #apply the same transform to both frame and mask\n",
        "                augmented = self.transform(image=frame, mask=obj_mask)\n",
        "                frame = augmented['image']\n",
        "                obj_mask = augmented['mask']\n",
        "            else:\n",
        "                #default transformation\n",
        "                frame = transforms.ToTensor()(frame)\n",
        "                obj_mask = torch.from_numpy(obj_mask)\n",
        "\n",
        "            frames.append(frame)\n",
        "            masks.append(obj_mask)\n",
        "\n",
        "        #pad sequences if needed\n",
        "        while len(frames) < self.max_seq_len:\n",
        "            frames.append(torch.zeros_like(frames[0]) if frames else torch.zeros(3, 384, 384))\n",
        "            masks.append(torch.zeros_like(masks[0]) if masks else torch.zeros(384, 384))\n",
        "\n",
        "        #tokenize expression\n",
        "        encoded_text = self.tokenizer(\n",
        "            expression,\n",
        "            padding='max_length',\n",
        "            max_length=20,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'frames': torch.stack(frames),\n",
        "            'masks': torch.stack(masks),\n",
        "            'text_ids': encoded_text['input_ids'].squeeze(0),\n",
        "            'text_mask': encoded_text['attention_mask'].squeeze(0),\n",
        "            'expression': expression,\n",
        "            'video_id': video_id,\n",
        "            'obj_id': anno_obj_id,\n",
        "            'actual_obj_id': actual_obj_id\n",
        "        }\n",
        "\n",
        "#create data loaders with augmentation\n",
        "def get_data_loaders(data_root, train_annotations, val_annotations, id_mapping, batch_size=4):\n",
        "    #training transforms with augmentation\n",
        "    train_transform = A.Compose([\n",
        "        A.Resize(height=384, width=384),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.ShiftScaleRotate(p=0.2),\n",
        "        A.RandomBrightnessContrast(p=0.2),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "    #validation transforms without augmentation\n",
        "    val_transform = A.Compose([\n",
        "        A.Resize(height=384, width=384),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "    train_dataset = RefDAVISDataset(data_root, train_annotations, id_mapping, transform=train_transform)\n",
        "    val_dataset = RefDAVISDataset(data_root, val_annotations, id_mapping, transform=val_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "#create the data loaders\n",
        "train_loader, val_loader = get_data_loaders(\n",
        "    '/content/drive/MyDrive/RVOS_VR_Project/data/ref-davis',\n",
        "    train_annotations,\n",
        "    val_annotations,\n",
        "    object_id_mapping,\n",
        "    batch_size=2\n",
        ")\n",
        "\n",
        "#check a batch from the data loader\n",
        "batch = next(iter(train_loader))\n",
        "print(f\"Batch shapes:\")\n",
        "print(f\"  Frames: {batch['frames'].shape}\")\n",
        "print(f\"  Masks: {batch['masks'].shape}\")\n",
        "print(f\"  Text IDs: {batch['text_ids'].shape}\")\n",
        "print(f\"  Text Mask: {batch['text_mask'].shape}\")\n",
        "\n",
        "#visualize a sample from the batch\n",
        "def visualize_batch_sample(batch, sample_idx=0):\n",
        "    \"\"\"Visualize a sample from a batch.\"\"\"\n",
        "    frames = batch['frames'][sample_idx].cpu()  # [T, C, H, W]\n",
        "    masks = batch['masks'][sample_idx].cpu()    # [T, H, W]\n",
        "    expression = batch['expression'][sample_idx]\n",
        "    video_id = batch['video_id'][sample_idx]\n",
        "    obj_id = batch['obj_id'][sample_idx]\n",
        "    actual_obj_id = batch['actual_obj_id'][sample_idx]\n",
        "\n",
        "    print(f\"Video: {video_id}\")\n",
        "    print(f\"Expression: {expression}\")\n",
        "    print(f\"Annotation Object ID: {obj_id}\")\n",
        "    print(f\"Actual Object ID: {actual_obj_id}\")\n",
        "\n",
        "    #denormalize frames\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
        "    frames = frames * std + mean\n",
        "    frames = frames.permute(0, 2, 3, 1).numpy()  # [T, H, W, C]\n",
        "    frames = np.clip(frames, 0, 1)\n",
        "\n",
        "    masks = masks.numpy()  # [T, H, W]\n",
        "\n",
        "    #display frames and masks\n",
        "    fig, axes = plt.subplots(frames.shape[0], 2, figsize=(10, 4*frames.shape[0]))\n",
        "\n",
        "    for i in range(frames.shape[0]):\n",
        "        #frame\n",
        "        axes[i, 0].imshow(frames[i])\n",
        "        axes[i, 0].set_title(f\"Frame {i}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        #mask overlay\n",
        "        axes[i, 1].imshow(frames[i])\n",
        "        axes[i, 1].imshow(masks[i], alpha=0.5, cmap='cool')\n",
        "        axes[i, 1].set_title(f\"Mask: {expression}\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#visualize a sample from the training batch\n",
        "visualize_batch_sample(batch)\n",
        "\n",
        "#check a batch from the validation loader\n",
        "val_batch = next(iter(val_loader))\n",
        "print(f\"\\nValidation batch shapes:\")\n",
        "print(f\"  Frames: {val_batch['frames'].shape}\")\n",
        "print(f\"  Masks: {val_batch['masks'].shape}\")\n",
        "print(f\"  Text IDs: {val_batch['text_ids'].shape}\")\n",
        "print(f\"  Text Mask: {val_batch['text_mask'].shape}\")\n",
        "\n",
        "#visualize a sample from the validation batch\n",
        "visualize_batch_sample(val_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ceb2GIyQv4w7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from transformers import BertModel\n",
        "from einops import rearrange\n",
        "\n",
        "class VisualBackbone(nn.Module):\n",
        "    def __init__(self, backbone_type='resnet50', pretrained=True):\n",
        "        super(VisualBackbone, self).__init__()\n",
        "        if backbone_type == 'resnet50':\n",
        "            resnet = models.resnet50(pretrained=pretrained)\n",
        "            self.out_channels = 2048\n",
        "        else:\n",
        "            resnet = models.resnet18(pretrained=pretrained)\n",
        "            self.out_channels = 512\n",
        "\n",
        "        #use feature layers instead of just removing last two layers\n",
        "        self.backbone = nn.Sequential(\n",
        "            resnet.conv1,\n",
        "            resnet.bn1,\n",
        "            resnet.relu,\n",
        "            resnet.maxpool,\n",
        "            resnet.layer1,\n",
        "            resnet.layer2,\n",
        "            resnet.layer3,\n",
        "            resnet.layer4\n",
        "        )\n",
        "\n",
        "        #additional feature refinement\n",
        "        self.feature_refine = nn.Conv2d(self.out_channels, self.out_channels // 2, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, t, c, h, w = x.shape\n",
        "        x = rearrange(x, 'b t c h w -> (b t) c h w')\n",
        "        features = self.backbone(x)\n",
        "        features = self.feature_refine(features)\n",
        "        features = rearrange(features, '(b t) c h w -> b t c h w', b=b, t=t)\n",
        "        return features\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, pooling='cls'):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.out_channels = 768\n",
        "        self.pooling = pooling\n",
        "\n",
        "        #optional additional projection\n",
        "        self.text_projection = nn.Sequential(\n",
        "            nn.Linear(self.out_channels, self.out_channels),\n",
        "            nn.LayerNorm(self.out_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        if self.pooling == 'cls':\n",
        "            text_features = outputs.last_hidden_state[:, 0]\n",
        "        elif self.pooling == 'mean':\n",
        "            text_features = (outputs.last_hidden_state * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1).unsqueeze(1)\n",
        "\n",
        "        text_features = self.text_projection(text_features)\n",
        "        return text_features\n",
        "\n",
        "class CrossModalFusion(nn.Module):\n",
        "    def __init__(self, visual_dim, text_dim, hidden_dim=256, num_heads=8):\n",
        "        super(CrossModalFusion, self).__init__()\n",
        "        self.visual_proj = nn.Sequential(\n",
        "            nn.Conv2d(visual_dim, hidden_dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "        self.text_proj = nn.Sequential(\n",
        "            nn.Linear(text_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, dropout=0.1)\n",
        "        self.norm = nn.LayerNorm(hidden_dim)\n",
        "        self.gamma = nn.Parameter(torch.ones(1))\n",
        "\n",
        "        #residual connection\n",
        "        self.residual_proj = nn.Conv2d(hidden_dim, hidden_dim, kernel_size=1)\n",
        "\n",
        "    def forward(self, visual_feat, text_feat):\n",
        "        b, t, _, h, w = visual_feat.shape\n",
        "        visual_feat = rearrange(visual_feat, 'b t c h w -> (b t) c h w')\n",
        "        visual_proj = self.visual_proj(visual_feat)\n",
        "        text_proj = self.text_proj(text_feat)\n",
        "\n",
        "        text_proj = text_proj.unsqueeze(1).expand(-1, t, -1)\n",
        "        text_proj = rearrange(text_proj, 'b t d -> (b t) d').unsqueeze(0)\n",
        "\n",
        "        visual_proj_flat = rearrange(visual_proj, '(b t) d h w -> (h w) (b t) d', b=b, t=t)\n",
        "        attn_output, _ = self.multihead_attn(visual_proj_flat, text_proj, text_proj)\n",
        "\n",
        "        attn_output = rearrange(attn_output, '(h w) (b t) d -> b t d h w', b=b, t=t, h=h, w=w)\n",
        "        visual_proj_reshaped = rearrange(visual_proj, '(b t) d h w -> b t d h w', b=b, t=t)\n",
        "\n",
        "        residual = self.residual_proj(rearrange(visual_proj_reshaped, 'b t d h w -> (b t) d h w'))\n",
        "        residual = rearrange(residual, '(b t) d h w -> b t d h w', b=b, t=t)\n",
        "\n",
        "        fused_feat = self.gamma * attn_output + visual_proj_reshaped + residual\n",
        "        fused_feat = rearrange(fused_feat, 'b t d h w -> (b t h w) d')\n",
        "        fused_feat = self.norm(fused_feat)\n",
        "        fused_feat = rearrange(fused_feat, '(b t h w) d -> b t d h w', b=b, t=t, h=h, w=w)\n",
        "\n",
        "        return fused_feat\n",
        "\n",
        "class TemporalModule(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_dim=256):\n",
        "        super(TemporalModule, self).__init__()\n",
        "        #use 3D Convolutions with more layers and deeper architecture\n",
        "        self.temporal_conv = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, hidden_dim, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
        "            nn.BatchNorm3d(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Conv3d(hidden_dim, hidden_dim * 2, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
        "            nn.BatchNorm3d(hidden_dim * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Conv3d(hidden_dim * 2, hidden_dim, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
        "            nn.BatchNorm3d(hidden_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        #optional residual connection\n",
        "        self.residual = nn.Conv3d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = rearrange(x, 'b t c h w -> b c t h w')\n",
        "        residual = self.residual(rearrange(x, 'b c t h w -> b c t h w'))\n",
        "        x = self.temporal_conv(x) + residual\n",
        "        x = rearrange(x, 'b c t h w -> b t c h w')\n",
        "        return x\n",
        "\n",
        "class SegmentationHead(nn.Module):\n",
        "    def __init__(self, in_channels, mid_channels=256, out_channels=1):\n",
        "        super(SegmentationHead, self).__init__()\n",
        "        self.segmentation_head = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Conv2d(mid_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, t, c, h, w = x.shape\n",
        "        x = rearrange(x, 'b t c h w -> (b t) c h w')\n",
        "        x = self.segmentation_head(x)\n",
        "        x = rearrange(x, '(b t) c h w -> b t c h w', b=b, t=t)\n",
        "        return x\n",
        "\n",
        "class RVOSModel(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, backbone_type='resnet50'):\n",
        "        super(RVOSModel, self).__init__()\n",
        "        self.visual_backbone = VisualBackbone(backbone_type=backbone_type)\n",
        "        visual_dim = self.visual_backbone.out_channels // 2\n",
        "\n",
        "        self.text_encoder = TextEncoder(pooling='mean')\n",
        "        text_dim = self.text_encoder.out_channels\n",
        "\n",
        "        self.fusion = CrossModalFusion(visual_dim, text_dim, hidden_dim)\n",
        "        self.temporal = TemporalModule(hidden_dim, hidden_dim)\n",
        "        self.segmentation_head = SegmentationHead(hidden_dim, hidden_dim, out_channels=1)\n",
        "\n",
        "        #adjustable upsampling\n",
        "        self.upsample = nn.Upsample(scale_factor=32, mode='bilinear', align_corners=False)\n",
        "\n",
        "    def forward(self, frames, text_ids, text_mask):\n",
        "        visual_features = self.visual_backbone(frames)\n",
        "        text_features = self.text_encoder(text_ids, text_mask)\n",
        "\n",
        "        fused_features = self.fusion(visual_features, text_features)\n",
        "        temporal_features = self.temporal(fused_features)\n",
        "\n",
        "        masks = self.segmentation_head(temporal_features)\n",
        "\n",
        "        b, t, c, h, w = masks.shape\n",
        "        masks = rearrange(masks, 'b t c h w -> (b t) c h w')\n",
        "        masks = self.upsample(masks)\n",
        "        masks = rearrange(masks, '(b t) c h w -> b t c h w', b=b, t=t)\n",
        "\n",
        "        return masks\n",
        "\n",
        "#initialize the model\n",
        "model = RVOSModel(backbone_type='resnet50')\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh75GcrxwWEl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "#to avoid memory fragmentation\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "#define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class RVOSLoss(nn.Module):\n",
        "    def __init__(self, bce_weight=0.3, dice_weight=0.6, focal_weight=0.1):\n",
        "        super(RVOSLoss, self).__init__()\n",
        "        self.bce_weight = bce_weight\n",
        "        self.dice_weight = dice_weight\n",
        "        self.focal_weight = focal_weight\n",
        "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        b, t, c, h, w = pred.shape\n",
        "        if target.dim() == 4:\n",
        "            target = target.unsqueeze(2)\n",
        "        if target.shape[3] != h or target.shape[4] != w:\n",
        "            target = F.interpolate(\n",
        "                target.reshape(b*t, 1, target.shape[3], target.shape[4]),\n",
        "                size=(h, w),\n",
        "                mode='nearest'\n",
        "            ).reshape(b, t, 1, h, w)\n",
        "\n",
        "        pred_flat = pred.reshape(b * t, c, h, w)\n",
        "        target_flat = target.reshape(b * t, 1, h, w)\n",
        "\n",
        "        #BCE Loss\n",
        "        bce = self.bce_loss(pred_flat, target_flat)\n",
        "\n",
        "        #Dice Loss with smoother calculation\n",
        "        pred_sigmoid = torch.sigmoid(pred_flat)\n",
        "        smooth = 1e-6\n",
        "        intersection = (pred_sigmoid * target_flat).sum(dim=(2, 3))\n",
        "        union = pred_sigmoid.sum(dim=(2, 3)) + target_flat.sum(dim=(2, 3))\n",
        "        dice = 1 - ((2.0 * intersection + smooth) / (union + smooth))\n",
        "        dice = dice.mean()\n",
        "\n",
        "        #Focal Loss\n",
        "        focal = self.focal_loss(pred_flat, target_flat)\n",
        "\n",
        "        #combine losses\n",
        "        loss = (self.bce_weight * bce +\n",
        "                self.dice_weight * dice +\n",
        "                self.focal_weight * focal)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def focal_loss(self, pred, target, alpha=0.25, gamma=2):\n",
        "        bce = F.binary_cross_entropy_with_logits(pred, target, reduction='none')\n",
        "        pred_sigmoid = torch.sigmoid(pred)\n",
        "        p_t = pred_sigmoid * target + (1 - pred_sigmoid) * (1 - target)\n",
        "\n",
        "        loss = bce * ((1 - p_t) ** gamma)\n",
        "\n",
        "        if alpha >= 0:\n",
        "            alpha_t = alpha * target + (1 - alpha) * (1 - target)\n",
        "            loss = alpha_t * loss\n",
        "\n",
        "        return loss.mean()\n",
        "\n",
        "def calculate_iou(pred, target, threshold=0.5):\n",
        "    b, t, c, h, w = pred.shape\n",
        "    if target.dim() == 4:\n",
        "        target = target.unsqueeze(2)\n",
        "    if target.shape[3] != h or target.shape[4] != w:\n",
        "        target = F.interpolate(\n",
        "            target.reshape(b*t, 1, target.shape[3], target.shape[4]),\n",
        "            size=(h, w),\n",
        "            mode='nearest'\n",
        "        ).reshape(b, t, 1, h, w)\n",
        "\n",
        "    pred = (pred > threshold).float()\n",
        "\n",
        "    #prevent division by zero\n",
        "    epsilon = 1e-7\n",
        "\n",
        "    intersection = (pred * target).sum((2, 3, 4))\n",
        "    union = pred.sum((2, 3, 4)) + target.sum((2, 3, 4)) - intersection\n",
        "    union = torch.clamp(union, min=epsilon)\n",
        "\n",
        "    iou = intersection / union\n",
        "    return iou.mean()\n",
        "\n",
        "def train_one_epoch(model, train_loader, criterion, optimizer, device, scaler,\n",
        "                    accumulation_steps=4, clip_grad_norm=1.0):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_iou = 0\n",
        "    num_batches = len(train_loader)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #enhanced logging and tracking\n",
        "    total_gradient_norm = 0\n",
        "\n",
        "    with tqdm(total=num_batches, desc=\"Training\") as pbar:\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            frames = batch['frames'].to(device)\n",
        "            masks = batch['masks'].to(device)\n",
        "            text_ids = batch['text_ids'].to(device)\n",
        "            text_mask = batch['text_mask'].to(device)\n",
        "\n",
        "            #mixed precision training\n",
        "            with autocast('cuda', dtype=torch.float16):\n",
        "                outputs = model(frames, text_ids, text_mask)\n",
        "                loss = criterion(outputs, masks)\n",
        "\n",
        "            #gradient accumulation with scaled loss\n",
        "            scaler.scale(loss / accumulation_steps).backward()\n",
        "\n",
        "            #check if it's time to step\n",
        "            if (i + 1) % accumulation_steps == 0:\n",
        "                #gradient norm clipping\n",
        "                scaler.unscale_(optimizer)\n",
        "                if clip_grad_norm > 0:\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
        "                    total_gradient_norm += grad_norm.item()\n",
        "\n",
        "                #optimizer step\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            #IoU calculation\n",
        "            with torch.no_grad():\n",
        "                iou = calculate_iou(torch.sigmoid(outputs), masks)\n",
        "\n",
        "            #tracking metrics\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_iou += iou.item()\n",
        "\n",
        "            #Progress bar update\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix({\n",
        "                \"Loss\": f\"{loss.item():.4f}\",\n",
        "                \"IoU\": f\"{iou.item():.4f}\",\n",
        "                \"Avg Grad Norm\": f\"{total_gradient_norm / max(1, (i+1)/accumulation_steps):.4f}\"\n",
        "            })\n",
        "\n",
        "            #Memory management\n",
        "            del outputs, loss\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        if (num_batches % accumulation_steps) != 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            if clip_grad_norm > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "    avg_loss = epoch_loss / num_batches\n",
        "    avg_iou = epoch_iou / num_batches\n",
        "    avg_grad_norm = total_gradient_norm / (num_batches / accumulation_steps)\n",
        "\n",
        "    return avg_loss, avg_iou, avg_grad_norm\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_iou = 0\n",
        "    num_batches = len(val_loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with tqdm(total=num_batches, desc=\"Validation\") as pbar:\n",
        "            for batch in val_loader:\n",
        "                frames = batch['frames'].to(device)\n",
        "                masks = batch['masks'].to(device)\n",
        "                text_ids = batch['text_ids'].to(device)\n",
        "                text_mask = batch['text_mask'].to(device)\n",
        "\n",
        "                with autocast('cuda', dtype=torch.float16):\n",
        "                    outputs = model(frames, text_ids, text_mask)\n",
        "                    loss = criterion(outputs, masks)\n",
        "\n",
        "                iou = calculate_iou(torch.sigmoid(outputs), masks)\n",
        "                val_loss += loss.item()\n",
        "                val_iou += iou.item()\n",
        "\n",
        "                pbar.update(1)\n",
        "                pbar.set_postfix({\n",
        "                    \"Loss\": f\"{loss.item():.4f}\",\n",
        "                    \"IoU\": f\"{iou.item():.4f}\"\n",
        "                })\n",
        "\n",
        "                del outputs, loss\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = val_loss / num_batches\n",
        "    avg_iou = val_iou / num_batches\n",
        "    return avg_loss, avg_iou\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=30, lr=5e-4, device=device):\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = RVOSLoss()\n",
        "\n",
        "    #optimizer with weight decay and adaptive learning\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=1e-4,\n",
        "        betas=(0.9, 0.999)\n",
        "    )\n",
        "\n",
        "    #cosine Annealing with Warm Restarts\n",
        "    scheduler = CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=5,  #initial restart period\n",
        "        T_mult=2,  #multiplicative factor for subsequent restart periods\n",
        "        eta_min=1e-6  #minimum learning rate\n",
        "    )\n",
        "\n",
        "    #Mixed precision training\n",
        "    scaler = GradScaler('cuda')\n",
        "\n",
        "    #model checkpoint management\n",
        "    best_val_iou = 0\n",
        "    best_model_path = '/content/drive/MyDrive/RVOS_VR_Project/checkpoints/best_model.pth'\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        #learning rate logging\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "        #training phase\n",
        "        train_loss, train_iou, train_grad_norm = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, device, scaler\n",
        "        )\n",
        "\n",
        "        #validation phase\n",
        "        val_loss, val_iou = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        #learning rate scheduling\n",
        "        scheduler.step()\n",
        "\n",
        "        #epoch metrics\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train IoU: {train_iou:.4f}, Train Grad Norm: {train_grad_norm:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val IoU: {val_iou:.4f}\")\n",
        "\n",
        "        #model checkpointing\n",
        "        if val_iou > best_val_iou:\n",
        "            best_val_iou = val_iou\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_iou': val_iou,\n",
        "                'val_loss': val_loss,\n",
        "            }, best_model_path)\n",
        "            print(f\"Saved best model with Val IoU: {val_iou:.4f}\")\n",
        "\n",
        "    #load and return the best model\n",
        "    checkpoint = torch.load(best_model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Loaded best model from epoch {checkpoint['epoch']+1} with Val IoU: {checkpoint['val_iou']:.4f}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lft_y3W46hhL"
      },
      "outputs": [],
      "source": [
        "!mkdir '/content/drive/MyDrive/RVOS_VR_Project/checkpoints'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8eUxA0vweb7"
      },
      "outputs": [],
      "source": [
        "model = train_model(model, train_loader, val_loader, num_epochs=50, lr=1e-4, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYz_Zhb1Bc8p"
      },
      "outputs": [],
      "source": [
        "#load the best model\n",
        "best_model_path = '/content/drive/MyDrive/RVOS_VR_Project/checkpoints/best_model.pth'\n",
        "checkpoint = torch.load(best_model_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "print(f\"Loaded best model from epoch {checkpoint['epoch']+1} with Val IoU: {checkpoint['val_iou']:.4f}\")\n",
        "\n",
        "def visualize_specific_videos(model, dataloader, device, video_names):\n",
        "    \"\"\"Visualize model predictions on specific videos.\"\"\"\n",
        "    model.eval()\n",
        "    video_batches = {}\n",
        "    for batch in dataloader:\n",
        "        batch_video_ids = batch['video_id']\n",
        "        for i, vid in enumerate(batch_video_ids):\n",
        "            if vid in video_names and vid not in video_batches:\n",
        "                single_batch = {\n",
        "                    'frames': batch['frames'][i:i+1],\n",
        "                    'masks': batch['masks'][i:i+1],\n",
        "                    'text_ids': batch['text_ids'][i:i+1],\n",
        "                    'text_mask': batch['text_mask'][i:i+1],\n",
        "                    'expression': [batch['expression'][i]],\n",
        "                    'video_id': [batch['video_id'][i]]\n",
        "                }\n",
        "                video_batches[vid] = single_batch\n",
        "                if len(video_batches) == len(video_names):\n",
        "                    break\n",
        "        if len(video_batches) == len(video_names):\n",
        "            break\n",
        "\n",
        "    for video_id, batch in video_batches.items():\n",
        "        print(f\"\\nProcessing video: {video_id}\")\n",
        "\n",
        "        frames = batch['frames'].to(device)\n",
        "        masks = batch['masks'].to(device)\n",
        "        text_ids = batch['text_ids'].to(device)\n",
        "        text_mask = batch['text_mask'].to(device)\n",
        "        expressions = batch['expression']\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(frames, text_ids, text_mask)\n",
        "            pred_masks = torch.sigmoid(outputs) > 0.5\n",
        "\n",
        "        #visualize\n",
        "        sample_frames = frames[0].cpu()  # [T, C, H, W]\n",
        "        sample_masks = masks[0].cpu()    # [T, H, W]\n",
        "        sample_preds = pred_masks[0, :, 0].cpu()  # [T, H, W]\n",
        "        expression = expressions[0]\n",
        "\n",
        "        #denormalize frames\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
        "        sample_frames = sample_frames * std + mean\n",
        "        sample_frames = sample_frames.permute(0, 2, 3, 1).numpy()  # [T, H, W, C]\n",
        "        sample_frames = np.clip(sample_frames, 0, 1)\n",
        "\n",
        "        #convert masks to numpy\n",
        "        sample_masks = sample_masks.numpy()\n",
        "        sample_preds = sample_preds.numpy()\n",
        "\n",
        "        #frames and masks\n",
        "        fig, axes = plt.subplots(sample_frames.shape[0], 2, figsize=(15, 4*sample_frames.shape[0]))\n",
        "\n",
        "        #handle case with only one frame\n",
        "        if sample_frames.shape[0] == 1:\n",
        "            axes = np.array([axes])\n",
        "\n",
        "        for t in range(sample_frames.shape[0]):\n",
        "            #frame\n",
        "            axes[t, 0].imshow(sample_frames[t])\n",
        "            axes[t, 0].set_title(f\"Frame {t}\")\n",
        "            axes[t, 0].axis('off')\n",
        "\n",
        "            #predicted mask\n",
        "            axes[t, 1].imshow(sample_frames[t])\n",
        "            axes[t, 1].imshow(sample_preds[t], alpha=0.5, cmap='cool')\n",
        "            axes[t, 1].set_title(f\"Prediction\")\n",
        "            axes[t, 1].axis('off')\n",
        "\n",
        "        plt.suptitle(f\"Video: {video_id}, Expression: '{expression}'\", fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.subplots_adjust(top=0.95)\n",
        "        plt.show()\n",
        "available_videos = list(val_annotations.keys())\n",
        "print(\"Available videos in validation set:\")\n",
        "for i, video in enumerate(available_videos):\n",
        "    print(f\"{i+1}. {video}\")\n",
        "\n",
        "print(\"\\nSelect videos to visualize (comma-separated numbers, e.g., '1,3,5'):\")\n",
        "selection = input(f\"Enter video numbers (1-{len(available_videos)}): \")\n",
        "selected_indices = [int(idx.strip()) - 1 for idx in selection.split(',')]\n",
        "selected_videos = [available_videos[idx] for idx in selected_indices if 0 <= idx < len(available_videos)]\n",
        "\n",
        "print(f\"\\nSelected videos: {selected_videos}\")\n",
        "visualize_specific_videos(model, val_loader, device, selected_videos)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
